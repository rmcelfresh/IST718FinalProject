{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6038,
     "status": "ok",
     "timestamp": 1557796088593,
     "user": {
      "displayName": "Randy Geszvain",
      "photoUrl": "",
      "userId": "01626251001813095820"
     },
     "user_tz": 300
    },
    "id": "6LIyR0pNgSBi",
    "outputId": "a7a9333f-18ec-4b3d-df74-e58516627f32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in /Users/randallgeszvain/anaconda3/lib/python3.7/site-packages (3.7.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/randallgeszvain/anaconda3/lib/python3.7/site-packages (from tweepy) (1.2.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/randallgeszvain/anaconda3/lib/python3.7/site-packages (from tweepy) (1.12.0)\n",
      "Requirement already satisfied: requests>=2.11.1 in /Users/randallgeszvain/anaconda3/lib/python3.7/site-packages (from tweepy) (2.21.0)\n",
      "Requirement already satisfied: PySocks>=1.5.7 in /Users/randallgeszvain/anaconda3/lib/python3.7/site-packages (from tweepy) (1.7.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/randallgeszvain/anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->tweepy) (3.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/randallgeszvain/anaconda3/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (2019.3.9)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /Users/randallgeszvain/anaconda3/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/randallgeszvain/anaconda3/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/randallgeszvain/anaconda3/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: preprocessor in /Users/randallgeszvain/anaconda3/lib/python3.7/site-packages (1.1.3)\n",
      "Requirement already satisfied: textblob in /Users/randallgeszvain/anaconda3/lib/python3.7/site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in /Users/randallgeszvain/anaconda3/lib/python3.7/site-packages (from textblob) (3.4.1)\n",
      "Requirement already satisfied: six in /Users/randallgeszvain/anaconda3/lib/python3.7/site-packages (from nltk>=3.1->textblob) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy\n",
    "!pip install preprocessor\n",
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6141,
     "status": "ok",
     "timestamp": 1557796088708,
     "user": {
      "displayName": "Randy Geszvain",
      "photoUrl": "",
      "userId": "01626251001813095820"
     },
     "user_tz": 300
    },
    "id": "UfgITtH0gMoc",
    "outputId": "e6fcf129-8333-45eb-cc59-8b6e79685ab7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/randallgeszvain/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/randallgeszvain/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/randallgeszvain/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tweepy\n",
    "import re\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "import preprocessor as p\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# importing libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import time\n",
    "\n",
    "import pandas as pd #Importing the PANDAS python library\n",
    "import numpy as np #importing Numpy\n",
    "%matplotlib inline \n",
    "\n",
    "#from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer #initiating VADER instance\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dm8E0GT2gYPS"
   },
   "outputs": [],
   "source": [
    "#Twitter credentials for the app\n",
    "# Consume:\n",
    "CONSUMER_KEY    = '9BKdl1VWnF0HAatcsLnlAMRgy'\n",
    "CONSUMER_SECRET = 'X4AhMhjkdqLsuOTdpVmzOhOh87oM44KBLwa0sYBA0BQWCT6Y3L'\n",
    "\n",
    "# Access:\n",
    "ACCESS_TOKEN  = '1124830893934370816-lOoMmIY66jAfrln1kWUOgzRgYRFOsU'\n",
    "ACCESS_SECRET = 'oSTrJkdwGRVjLtTTP8p1jQLt7yYliO95Uxez8XQqs9oX1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vEaUKjNqgcv5"
   },
   "outputs": [],
   "source": [
    "#pass twitter credentials to tweepy\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TczulEMIgtXp"
   },
   "outputs": [],
   "source": [
    "#declare file paths as follows for three files\n",
    "AgeofUltron_tweets = \"AgeofUltron_data.csv\"\n",
    "Antman_tweets = \"Antman_data.csv\"\n",
    "CivilWar_tweets = \"CivilWar_data.csv\"\n",
    "DoctorStrange_tweets = \"DoctorStrange_data.csv\"\n",
    "GuardiansoftheGalaxy_tweets = \"GuardiansoftheGalaxy_data.csv\"\n",
    "SpidermanHomecoming_tweets = \"SpidermanHomecoming_data.csv\"\n",
    "ThorRagnarok_tweets = \"ThorRagnarok_data.csv\"\n",
    "BlackPanther_tweets = \"BlackPanther_data.csv\"\n",
    "AvengersInfinityWar_tweets = \"AvengersInfinityWar_data.csv\"\n",
    "Antmanthewasp_tweets = \"AntMantheWasp_data.csv\"\n",
    "AvengersEndgame_tweets = \"AvengersEndgame_data.csv\"\n",
    "RogueOne_tweets = \"RogueOne_data.csv\"\n",
    "Solo_tweets = \"Solo_data.csv\"\n",
    "TheForceAwakens_tweets = \"TheForceAwakens_data.csv\"\n",
    "TheLastJedi_tweets = \"TheLastJedi_data.csv\"\n",
    "DawnofJustice_tweets = \"DawnofJustice_data.csv\"\n",
    "SuicideSquad_tweets = \"SuicideSquad_data.csv\"\n",
    "WonderWoman_tweets = \"WonderWoman_data.csv\"\n",
    "JusticeLeague_tweets = \"JusticeLeague_data.csv\"\n",
    "Aquaman_tweets = \"Aquaman_data.csv\"\n",
    "Shazam_tweets = \"Shazam_data.csv\"\n",
    "SpidermanFarfromhome_tweets = \"SpidermanFarfromhome_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ayMX8_nRgzE6"
   },
   "outputs": [],
   "source": [
    "#columns of the csv file\n",
    "COLS = ['id', 'created_at', 'source', 'original_text','clean_text', 'sentiment','polarity','subjectivity', 'lang',\n",
    "        'favorite_count', 'retweet_count', 'original_author', 'possibly_sensitive', 'hashtags',\n",
    "        'user_mentions', 'place', 'place_coord_boundaries']\n",
    " \n",
    "#set two date variables for date range\n",
    "start_date = '2015-01-01'\n",
    "end_date = '2019-06-12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dMoO8fW2g1HS"
   },
   "outputs": [],
   "source": [
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h954Pnefg3RB"
   },
   "outputs": [],
   "source": [
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XgdPtTSNg8jS"
   },
   "outputs": [],
   "source": [
    "#Emoji patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uijLpPDEg-cj"
   },
   "outputs": [],
   "source": [
    "#combine sad and happy emoticons\n",
    "emoticons = emoticons_happy.union(emoticons_sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "By6Fht7NhB7C"
   },
   "outputs": [],
   "source": [
    "#mrhod clean_tweets()\n",
    "def clean_tweets(tweet):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    #Add stop words\n",
    "    stop_words.add(\"AvengersEndgame\")\n",
    "    stop_words.add(\"Avengers\")\n",
    "    stop_words.add(\"Endgame\")\n",
    "    stop_words.add(\"Marvel\")\n",
    "    stop_words.add(\"Studio\")\n",
    "    stop_words.add(\"Avengersâ\")\n",
    "    word_tokens = word_tokenize(tweet)\n",
    " \n",
    "    #after tweepy preprocessing the colon left remain after removing mentions\n",
    "    #or RT sign in the beginning of the tweet\n",
    "    tweet = re.sub(r':', '', tweet)\n",
    "    tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    " \n",
    " \n",
    "    #remove emojis from tweet\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    " \n",
    "    #filter using NLTK library append it to a string\n",
    "    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_tweet = []\n",
    " \n",
    "    #looping through conditions\n",
    "    for w in word_tokens:\n",
    "        #check tokens against stop words , emoticons and punctuations\n",
    "        if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
    "            filtered_tweet.append(w)\n",
    "    return ' '.join(filtered_tweet)\n",
    "    #print(word_tokens)\n",
    "    #print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y9H6YySBhGaB"
   },
   "outputs": [],
   "source": [
    "#method write_tweets()\n",
    "def write_tweets(keyword, file):\n",
    "    # If the file exists, then read the existing data from the CSV file.\n",
    "    if os.path.exists(file):\n",
    "        df = pd.read_csv(file, header=0)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=COLS)\n",
    "    #page attribute in tweepy.cursor and iteration\n",
    "    for page in tweepy.Cursor(api.search, q=keyword,\n",
    "                              count=200, include_rts=False, since=start_date).pages(50):\n",
    "        for status in page:\n",
    "            new_entry = []\n",
    "            status = status._json\n",
    " \n",
    "            ## check whether the tweet is in english or skip to the next tweet\n",
    "            if status['lang'] != 'en':\n",
    "                continue\n",
    " \n",
    "            #when run the code, below code replaces the retweet amount and\n",
    "            #no of favorires that are changed since last download.\n",
    "            if status['created_at'] in df['created_at'].values:\n",
    "                i = df.loc[df['created_at'] == status['created_at']].index[0]\n",
    "                if status['favorite_count'] != df.at[i, 'favorite_count'] or \\\n",
    "                   status['retweet_count'] != df.at[i, 'retweet_count']:\n",
    "                    df.at[i, 'favorite_count'] = status['favorite_count']\n",
    "                    df.at[i, 'retweet_count'] = status['retweet_count']\n",
    "                continue\n",
    " \n",
    " \n",
    "           #tweepy preprocessing called for basic preprocessing\n",
    "            clean_text = status['text']\n",
    " \n",
    "            #call clean_tweet method for extra preprocessing\n",
    "            filtered_tweet=clean_tweets(clean_text)\n",
    " \n",
    "            #pass textBlob method for sentiment calculations\n",
    "            blob = TextBlob(filtered_tweet)\n",
    "            Sentiment = blob.sentiment\n",
    " \n",
    "            #seperate polarity and subjectivity in to two variables\n",
    "            polarity = Sentiment.polarity\n",
    "            subjectivity = Sentiment.subjectivity\n",
    " \n",
    "            #new entry append\n",
    "            new_entry += [status['id'], status['created_at'],\n",
    "                          status['source'], status['text'],filtered_tweet, Sentiment,polarity,subjectivity, status['lang'],\n",
    "                          status['favorite_count'], status['retweet_count']]\n",
    " \n",
    "            #to append original author of the tweet\n",
    "            new_entry.append(status['user']['screen_name'])\n",
    " \n",
    "            try:\n",
    "                is_sensitive = status['possibly_sensitive']\n",
    "            except KeyError:\n",
    "                is_sensitive = None\n",
    "            new_entry.append(is_sensitive)\n",
    " \n",
    "            # hashtagas and mentiones are saved using comma separted\n",
    "            hashtags = \", \".join([hashtag_item['text'] for hashtag_item in status['entities']['hashtags']])\n",
    "            new_entry.append(hashtags)\n",
    "            mentions = \", \".join([mention['screen_name'] for mention in status['entities']['user_mentions']])\n",
    "            new_entry.append(mentions)\n",
    " \n",
    "            #get location of the tweet if possible\n",
    "            try:\n",
    "                location = status['user']['location']\n",
    "            except TypeError:\n",
    "                location = ''\n",
    "            new_entry.append(location)\n",
    " \n",
    "            try:\n",
    "                coordinates = [coord for loc in status['place']['bounding_box']['coordinates'] for coord in loc]\n",
    "            except TypeError:\n",
    "                coordinates = None\n",
    "            new_entry.append(coordinates)\n",
    " \n",
    "            single_tweet_df = pd.DataFrame([new_entry], columns=COLS)\n",
    "            df = df.append(single_tweet_df, ignore_index=True)\n",
    "            csvFile = open(file, 'a' ,encoding='utf-8')\n",
    "    df.to_csv(csvFile, mode='a', columns=COLS, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CDmuZViBlhJa"
   },
   "outputs": [],
   "source": [
    "#declare keywords as a query for three categories\n",
    "#AgeofUltron_keywords = '#AgeofUltron OR #AvengersAgeofUltron'\n",
    "#Antman_keywords = '#Antman'\n",
    "#CivilWar_keywords = '#CivilWar OR #CaptainAmericaCivilWar'\n",
    "#DoctorStrange_keywords = '#DoctorStrange'\n",
    "#GuardiansoftheGalaxy_keywords = '#GuardiansoftheGalaxy'\n",
    "#SpidermanHomecoming_keywords = '#SpidermanHomecoming'\n",
    "#ThorRagnarok_keywords = '#ThorRagnarok OR #Thor'\n",
    "#BlackPanther_keywords = '#BlackPanther'\n",
    "#AvengersInfinityWar_keywords = '#InfinityWar OR #AvengersInfinityWar'\n",
    "#Antmanthewasp_keywords = '#Antmanthewasp OR #thewasp'\n",
    "#AvengersEndgame_keywords = '#AvengersEndgame OR #Endgame'\n",
    "#RogueOne_keywords = '#RogueOne OR #StarWarsRogueOne'\n",
    "#Solo_keywords = '#StarWarsSolo'\n",
    "#TheForceAwakens_keywords = '#TheForceAwakens OR #ForceAwakens'\n",
    "#TheLastJedi_keywords = '#TheLastJedi OR #LastJedi'\n",
    "#DawnofJustice_keywords = '#DawnofJustice OR #BatmanvSuperman OR #BatmanvsSuperman'\n",
    "#SuicideSquad_keywords = '#SuicideSquad'\n",
    "#WonderWoman_keywords = '#WonderWoman'\n",
    "#JusticeLeague_keywords = '#JusticeLeague'\n",
    "#Aquaman_keywords = '#Aquaman'\n",
    "#Shazam_keywords = '#Shazam'\n",
    "SpidermanFarfromhome_keywords = '#SpidermanFarfromhome'\n",
    "\n",
    "\n",
    "\n",
    "#call main method passing keywords and file path\n",
    "#write_tweets(AgeofUltron_keywords,  AgeofUltron_tweets)\n",
    "#write_tweets(Antman_keywords, Antman_tweets)\n",
    "#write_tweets(CivilWar_keywords, CivilWar_tweets)\n",
    "#write_tweets(DoctorStrange_keywords, DoctorStrange_tweets)\n",
    "#write_tweets(GuardiansoftheGalaxy_keywords, GuardiansoftheGalaxy_tweets)\n",
    "#write_tweets(SpidermanHomecoming_keywords, SpidermanHomecoming_tweets)\n",
    "#write_tweets(ThorRagnarok_keywords, ThorRagnarok_tweets)\n",
    "#write_tweets(BlackPanther_keywords, BlackPanther_tweets)\n",
    "#write_tweets(AvengersInfinityWar_keywords, AvengersInfinityWar_tweets)\n",
    "#write_tweets(Antmanthewasp_keywords, Antmanthewasp_tweets)\n",
    "#write_tweets(AvengersEndgame_keywords, AvengersEndgame_tweets)\n",
    "#write_tweets(RogueOne_keywords, RogueOne_tweets)\n",
    "#write_tweets(Solo_keywords, Solo_tweets)\n",
    "#write_tweets(TheForceAwakens_keywords, TheForceAwakens_tweets)\n",
    "#write_tweets(TheLastJedi_keywords, TheLastJedi_tweets)\n",
    "#write_tweets(DawnofJustice_keywords, DawnofJustice_tweets)\n",
    "#write_tweets(SuicideSquad_keywords, SuicideSquad_tweets)\n",
    "#write_tweets(WonderWoman_keywords, WonderWoman_tweets)\n",
    "#write_tweets(JusticeLeague_keywords, JusticeLeague_tweets)\n",
    "#write_tweets(Aquaman_keywords, Aquaman_tweets)\n",
    "#write_tweets(Shazam_keywords, Shazam_tweets)\n",
    "write_tweets(SpidermanFarfromhome_keywords, SpidermanFarfromhome_tweets)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Extracting Twitter Data - Avengers End Game.ipynb",
   "provenance": [
    {
     "file_id": "1L3sGgaRILQ3DJ-oap-J7vB-EuwmtZifz",
     "timestamp": 1557787952106
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
