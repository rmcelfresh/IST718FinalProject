{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2093,
     "status": "ok",
     "timestamp": 1557082481067,
     "user": {
      "displayName": "Randy Geszvain",
      "photoUrl": "",
      "userId": "01626251001813095820"
     },
     "user_tz": 300
    },
    "id": "gbGF5YrLPg-k",
    "outputId": "84e19096-2cbd-4fb5-fc43-e1c90ea59e03"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/randallgeszvain/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import time\n",
    "\n",
    "import pandas as pd #Importing the PANDAS python library\n",
    "import numpy as np #importing Numpy\n",
    "%matplotlib inline \n",
    "\n",
    "#from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer #initiating VADER instance\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 84118,
     "status": "ok",
     "timestamp": 1557082563101,
     "user": {
      "displayName": "Randy Geszvain",
      "photoUrl": "",
      "userId": "01626251001813095820"
     },
     "user_tz": 300
    },
    "id": "2jIShF4UWPCb",
    "outputId": "c9d818f9-c854-4414-a022-716a3a893735",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'review' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a2f411fb62c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mcontent_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'review' is not defined"
     ]
    }
   ],
   "source": [
    "AgeofUltron_rm=[]\n",
    "for cp in np.arange(1,40):\n",
    "    url = \"https://www.rottentomatoes.com/m/avengers_age_of_ultron/reviews/?page=\" + str(cp)\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    AgeofUltron_rm.extend([i.text for i in soup.find_all('div', attrs={'class': 'the_review'})])\n",
    "    time.sleep(3)\n",
    "    \n",
    "content_array = np.asarray(AgeofUltron_rm)\n",
    "sentences = pd.DataFrame(content_array)\n",
    "sentences.columns = ['text']\n",
    "sentences = sentences.replace('\\n','', regex=True)\n",
    "sentences['text'] = sentences['text'].str.strip()\n",
    "export = sentences.to_csv('AgeofUltron_rm.csv')\n",
    "\n",
    "print (\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "Antman_rm=[]\n",
    "for cp in np.arange(1,31):\n",
    "    url = \"https://www.rottentomatoes.com/m/antman/reviews/?page=\" + str(cp)\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    Antman_rm.extend([i.text for i in soup.find_all('div', attrs={'class': 'the_review'})])\n",
    "    time.sleep(3)\n",
    "    \n",
    "content_array = np.asarray(Antman_rm)\n",
    "sentences = pd.DataFrame(content_array)\n",
    "sentences.columns = ['text']\n",
    "sentences = sentences.replace('\\n','', regex=True)\n",
    "sentences['text'] = sentences['text'].str.strip()\n",
    "export = sentences.to_csv('Antman_rm.csv')\n",
    "\n",
    "print (\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "CivilWar_rm=[]\n",
    "for cp in np.arange(1,40):\n",
    "    url = \"https://www.rottentomatoes.com/m/captain_america_civil_war/reviews/?page=\" + str(cp)\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    CivilWar_rm.extend([i.text for i in soup.find_all('div', attrs={'class': 'the_review'})])\n",
    "    time.sleep(3)\n",
    "    \n",
    "content_array = np.asarray(CivilWar_rm)\n",
    "sentences = pd.DataFrame(content_array)\n",
    "sentences.columns = ['text']\n",
    "sentences = sentences.replace('\\n','', regex=True)\n",
    "sentences['text'] = sentences['text'].str.strip()\n",
    "export = sentences.to_csv('CivilWar_rm.csv')\n",
    "\n",
    "print (\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "DoctorStrange_rm=[]\n",
    "for cp in np.arange(1,35):\n",
    "    url = \"https://www.rottentomatoes.com/m/doctor_strange_2016/reviews/?page=\" + str(cp)\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    DoctorStrange_rm.extend([i.text for i in soup.find_all('div', attrs={'class': 'the_review'})])\n",
    "    time.sleep(3)\n",
    "    \n",
    "content_array = np.asarray(DoctorStrange_rm)\n",
    "sentences = pd.DataFrame(content_array)\n",
    "sentences.columns = ['text']\n",
    "sentences = sentences.replace('\\n','', regex=True)\n",
    "sentences['text'] = sentences['text'].str.strip()\n",
    "export = sentences.to_csv('DoctorStrange_rm.csv')\n",
    "\n",
    "print (\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "GuardiansoftheGalaxy_rm=[]\n",
    "for cp in np.arange(1,39):\n",
    "    url = \"https://www.rottentomatoes.com/m/guardians_of_the_galaxy_vol_2/reviews/?page=\" + str(cp)\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    GuardiansoftheGalaxy_rm.extend([i.text for i in soup.find_all('div', attrs={'class': 'the_review'})])\n",
    "    time.sleep(3)\n",
    "    \n",
    "content_array = np.asarray(GuardiansoftheGalaxy_rm)\n",
    "sentences = pd.DataFrame(content_array)\n",
    "sentences.columns = ['text']\n",
    "sentences = sentences.replace('\\n','', regex=True)\n",
    "sentences['text'] = sentences['text'].str.strip()\n",
    "export = sentences.to_csv('GuardiansoftheGalaxy_rm.csv')\n",
    "\n",
    "print (\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "SpidermanHomecoming_rm=[]\n",
    "for cp in np.arange(1,37):\n",
    "    url = \"https://www.rottentomatoes.com/m/spider_man_homecoming/reviews/?page=\" + str(cp)\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    SpidermanHomecoming_rm.extend([i.text for i in soup.find_all('div', attrs={'class': 'the_review'})])\n",
    "    time.sleep(3)\n",
    "    \n",
    "content_array = np.asarray(SpidermanHomecoming_rm)\n",
    "sentences = pd.DataFrame(content_array)\n",
    "sentences.columns = ['text']\n",
    "sentences = sentences.replace('\\n','', regex=True)\n",
    "sentences['text'] = sentences['text'].str.strip()\n",
    "export = sentences.to_csv('SpidermanHomecoming_rm.csv')\n",
    "\n",
    "print (\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "ThorRagnarok_rm=[]\n",
    "for cp in np.arange(1,39):\n",
    "    url = \"https://www.rottentomatoes.com/m/thor_ragnarok_2017/reviews/?page=\" + str(cp)\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    ThorRagnarok_rm.extend([i.text for i in soup.find_all('div', attrs={'class': 'the_review'})])\n",
    "    time.sleep(3)\n",
    "    \n",
    "content_array = np.asarray(ThorRagnarok_rm)\n",
    "sentences = pd.DataFrame(content_array)\n",
    "sentences.columns = ['text']\n",
    "sentences = sentences.replace('\\n','', regex=True)\n",
    "sentences['text'] = sentences['text'].str.strip()\n",
    "export = sentences.to_csv('ThorRagnarok_rm.csv')\n",
    "\n",
    "print (\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "BlackPanther_rm=[]\n",
    "for cp in np.arange(1,47):\n",
    "    url = \"https://www.rottentomatoes.com/m/black_panther_2018/reviews/?page=\" + str(cp)\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    BlackPanther_rm.extend([i.text for i in soup.find_all('div', attrs={'class': 'the_review'})])\n",
    "    time.sleep(3)\n",
    "    \n",
    "content_array = np.asarray(BlackPanther_rm)\n",
    "sentences = pd.DataFrame(content_array)\n",
    "sentences.columns = ['text']\n",
    "sentences = sentences.replace('\\n','', regex=True)\n",
    "sentences['text'] = sentences['text'].str.strip()\n",
    "export = sentences.to_csv('BlackPanther_rm.csv')\n",
    "\n",
    "print (\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "AvengersInfinityWar_rm=[]\n",
    "for cp in np.arange(1,44):\n",
    "    url = \"https://www.rottentomatoes.com/m/avengers_infinity_war/reviews/?page=\" + str(cp)\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    AvengersInfinityWar_rm.extend([i.text for i in soup.find_all('div', attrs={'class': 'the_review'})])\n",
    "    time.sleep(3)\n",
    "    \n",
    "content_array = np.asarray(AvengersInfinityWar_rm)\n",
    "sentences = pd.DataFrame(content_array)\n",
    "sentences.columns = ['text']\n",
    "sentences = sentences.replace('\\n','', regex=True)\n",
    "sentences['text'] = sentences['text'].str.strip()\n",
    "export = sentences.to_csv('AvengersInfinityWar_rm.csv')\n",
    "\n",
    "print (\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "Antmanthewasp_rm=[]\n",
    "for cp in np.arange(1,40):\n",
    "    url = \"https://www.rottentomatoes.com/m/ant_man_and_the_wasp/reviews/?page=\" + str(cp)\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    Antmanthewasp_rm.extend([i.text for i in soup.find_all('div', attrs={'class': 'the_review'})])\n",
    "    time.sleep(3)\n",
    "    \n",
    "content_array = np.asarray(Antmanthewasp_rm)\n",
    "sentences = pd.DataFrame(content_array)\n",
    "sentences.columns = ['text']\n",
    "sentences = sentences.replace('\\n','', regex=True)\n",
    "sentences['text'] = sentences['text'].str.strip()\n",
    "export = sentences.to_csv('Antmanthewasp_rm.csv')\n",
    "\n",
    "print (\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "AvengersEndgame_rm=[]\n",
    "for cp in np.arange(1,46):\n",
    "    url = \"https://www.rottentomatoes.com/m/avengers_endgame/reviews/?page=\" + str(cp)\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    AvengersEndgame_rm.extend([i.text for i in soup.find_all('div', attrs={'class': 'the_review'})])\n",
    "    time.sleep(3)\n",
    "    \n",
    "content_array = np.asarray(AvengersEndgame_rm)\n",
    "sentences = pd.DataFrame(content_array)\n",
    "sentences.columns = ['text']\n",
    "sentences = sentences.replace('\\n','', regex=True)\n",
    "sentences['text'] = sentences['text'].str.strip()\n",
    "export = sentences.to_csv('AvengersEndgame_rm.csv')\n",
    "\n",
    "print (\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "RogueOne_rm=[]\n",
    "for cp in np.arange(1,43):\n",
    "    url = \"https://www.rottentomatoes.com/m/rogue_one_a_star_wars_story/reviews/?page=\" + str(cp)\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    RogueOne_rm.extend([i.text for i in soup.find_all('div', attrs={'class': 'the_review'})])\n",
    "    time.sleep(3)\n",
    "    \n",
    "content_array = np.asarray(RogueOne_rm)\n",
    "sentences = pd.DataFrame(content_array)\n",
    "sentences.columns = ['text']\n",
    "sentences = sentences.replace('\\n','', regex=True)\n",
    "sentences['text'] = sentences['text'].str.strip()\n",
    "export = sentences.to_csv('RogueOne_rm.csv')\n",
    "\n",
    "print (\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "Solo_rm=[]\n",
    "for cp in np.arange(1,45):\n",
    "    url = \"https://www.rottentomatoes.com/m/solo_a_star_wars_story/reviews/?page=\" + str(cp)\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    Solo_rm.extend([i.text for i in soup.find_all('div', attrs={'class': 'the_review'})])\n",
    "    time.sleep(3)\n",
    "    \n",
    "content_array = np.asarray(Solo_rm)\n",
    "sentences = pd.DataFrame(content_array)\n",
    "sentences.columns = ['text']\n",
    "sentences = sentences.replace('\\n','', regex=True)\n",
    "sentences['text'] = sentences['text'].str.strip()\n",
    "export = sentences.to_csv('Solo_rm.csv')\n",
    "\n",
    "print (\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "TheForceAwakens_rm=[]\n",
    "for cp in np.arange(1,42):\n",
    "    url = \"https://www.rottentomatoes.com/m/star_wars_episode_vii_the_force_awakens/reviews/?page=\" + str(cp)\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    TheForceAwakens_rm.extend([i.text for i in soup.find_all('div', attrs={'class': 'the_review'})])\n",
    "    time.sleep(3)\n",
    "    \n",
    "content_array = np.asarray(TheForceAwakens_rm)\n",
    "sentences = pd.DataFrame(content_array)\n",
    "sentences.columns = ['text']\n",
    "sentences = sentences.replace('\\n','', regex=True)\n",
    "sentences['text'] = sentences['text'].str.strip()\n",
    "export = sentences.to_csv('TheForceAwakens_rm.csv')\n",
    "\n",
    "print (\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "TheLastJedi_rm=[]\n",
    "for cp in np.arange(1,44):\n",
    "    url = \"https://www.rottentomatoes.com/m/star_wars_the_last_jedi/reviews/?page=\" + str(cp)\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    TheLastJedi_rm.extend([i.text for i in soup.find_all('div', attrs={'class': 'the_review'})])\n",
    "    time.sleep(3)\n",
    "    \n",
    "content_array = np.asarray(TheLastJedi_rm)\n",
    "sentences = pd.DataFrame(content_array)\n",
    "sentences.columns = ['text']\n",
    "sentences = sentences.replace('\\n','', regex=True)\n",
    "sentences['text'] = sentences['text'].str.strip()\n",
    "export = sentences.to_csv('TheLastJedi_rm.csv')\n",
    "\n",
    "print (\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "DawnofJustice_rm=[]\n",
    "for cp in np.arange(1,42):\n",
    "    url = \"https://www.rottentomatoes.com/m/batman_v_superman_dawn_of_justice/reviews/?page=\" + str(cp)\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    DawnofJustice_rm.extend([i.text for i in soup.find_all('div', attrs={'class': 'the_review'})])\n",
    "    time.sleep(3)\n",
    "    \n",
    "content_array = np.asarray(DawnofJustice_rm)\n",
    "sentences = pd.DataFrame(content_array)\n",
    "sentences.columns = ['text']\n",
    "sentences = sentences.replace('\\n','', regex=True)\n",
    "sentences['text'] = sentences['text'].str.strip()\n",
    "export = sentences.to_csv('DawnofJustice_rm.csv')\n",
    "\n",
    "print (\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "SuicideSquad_rm=[]\n",
    "for cp in np.arange(1,37):\n",
    "    url = \"https://www.rottentomatoes.com/m/suicide_squad_2016/reviews/?page=\" + str(cp)\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    SuicideSquad_rm.extend([i.text for i in soup.find_all('div', attrs={'class': 'the_review'})])\n",
    "    time.sleep(3)\n",
    "    \n",
    "content_array = np.asarray(SuicideSquad_rm)\n",
    "sentences = pd.DataFrame(content_array)\n",
    "sentences.columns = ['text']\n",
    "sentences = sentences.replace('\\n','', regex=True)\n",
    "sentences['text'] = sentences['text'].str.strip()\n",
    "export = sentences.to_csv('SuicideSquad_rm.csv')\n",
    "\n",
    "print (\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "WonderWoman_rm=[]\n",
    "for cp in np.arange(1,43):\n",
    "    url = \"https://www.rottentomatoes.com/m/wonder_woman_2017/reviews/?page=\" + str(cp)\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    WonderWoman_rm.extend([i.text for i in soup.find_all('div', attrs={'class': 'the_review'})])\n",
    "    time.sleep(3)\n",
    "    \n",
    "content_array = np.asarray(WonderWoman_rm)\n",
    "sentences = pd.DataFrame(content_array)\n",
    "sentences.columns = ['text']\n",
    "sentences = sentences.replace('\\n','', regex=True)\n",
    "sentences['text'] = sentences['text'].str.strip()\n",
    "export = sentences.to_csv('WonderWoman_rm.csv')\n",
    "\n",
    "print (\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "JusticeLeague_rm=[]\n",
    "for cp in np.arange(1,37):\n",
    "    url = \"https://www.rottentomatoes.com/m/justice_league_2017/reviews/?page=\" + str(cp)\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    JusticeLeague_rm.extend([i.text for i in soup.find_all('div', attrs={'class': 'the_review'})])\n",
    "    time.sleep(3)\n",
    "    \n",
    "content_array = np.asarray(JusticeLeague_rm)\n",
    "sentences = pd.DataFrame(content_array)\n",
    "sentences.columns = ['text']\n",
    "sentences = sentences.replace('\\n','', regex=True)\n",
    "sentences['text'] = sentences['text'].str.strip()\n",
    "export = sentences.to_csv('JusticeLeague_rm.csv')\n",
    "\n",
    "print (\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "Aquaman_rm=[]\n",
    "for cp in np.arange(1,37):\n",
    "    url = \"https://www.rottentomatoes.com/m/aquaman_2018/reviews/?page=\" + str(cp)\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    Aquaman_rm.extend([i.text for i in soup.find_all('div', attrs={'class': 'the_review'})])\n",
    "    time.sleep(3)\n",
    "    \n",
    "content_array = np.asarray(Aquaman_rm)\n",
    "sentences = pd.DataFrame(content_array)\n",
    "sentences.columns = ['text']\n",
    "sentences = sentences.replace('\\n','', regex=True)\n",
    "sentences['text'] = sentences['text'].str.strip()\n",
    "export = sentences.to_csv('Aquaman_rm.csv')\n",
    "\n",
    "print (\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "Shazam_rm=[]\n",
    "for cp in np.arange(1,36):\n",
    "    url = \"https://www.rottentomatoes.com/m/shazam/reviews/?page=\" + str(cp)\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    Shazam_rm.extend([i.text for i in soup.find_all('div', attrs={'class': 'the_review'})])\n",
    "    time.sleep(3)\n",
    "    \n",
    "content_array = np.asarray(Shazam_rm)\n",
    "sentences = pd.DataFrame(content_array)\n",
    "sentences.columns = ['text']\n",
    "sentences = sentences.replace('\\n','', regex=True)\n",
    "sentences['text'] = sentences['text'].str.strip()\n",
    "export = sentences.to_csv('Shazam_rm.csv')\n",
    "\n",
    "print (\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Sentiment Analysis - Avengers End Game.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
